# -*- coding: utf-8 -*-
"""ISB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13yWg4K9NjNRDaCwwSCjtJ6jJ0-hY_3bO
"""

#Iterative Similarity Bagging
import pandas as pd
import numpy as np
from scipy.spatial.distance import squareform, pdist
from preprocessing import *

class iterative_similarity_bagging:
  def __init__(self,data=None, k=1,threshold_method='half_mean',iterations=1,distance_metric='euclidean'):
    self.data = data
    self.k = k
    self.threshold_method = threshold_method
    self.iterations = iterations
    self.distance_metric = distance_metric

  def compute_similarity(self, df):
    distances = pdist(df.values, metric=self.distance_metric)
    dist_matrix  = squareform(distances)
    dist_matrix_df = pd.DataFrame(dist_matrix, columns=df.index, index=df.index)

    return dist_matrix_df

  def get_threshold(self, df):
    threshold = 0
    if self.threshold_method == 'mean':
      threshold = df.values.mean()
    elif self.threshold_method == 'median':
      threshold = df.values.median()
    elif self.threshold_method == 'half_mean':
      threshold = df.values.mean() - (df.values.mean()/2)
    elif self.threshold_method == 'half_median':
      threshold = df.values.median() - (df.values.median()/2)
    elif self.threshold_method == 'Q1':
      threshold = np.percentile(df.values, 25)

    return threshold

  def features_by_bagging(self, data):
    k = self.k
    increment= k
    i =0
    selected_features= []
    for index in range(0,len(data.columns)):
      if i < len(data.columns):
        df_bag = data.iloc[:, i:k]
        df = df_bag.T
        df_Sim = self.compute_similarity(df)
        # threshold can be mean, median, or fixed number etc
        threshold= self.get_threshold(df_Sim)

        df_mask = np.triu(np.ones_like(df_Sim,dtype=bool))
        df_triu = df_Sim.mask(df_mask)
        iteration_to_drop_features = [col for col in df_triu.columns if any(df_triu[col] < threshold)]
        iteration_selected_features = df_bag.drop(iteration_to_drop_features,axis=1).columns.tolist()
        selected_features += list(set(iteration_selected_features))

        i = i + increment
        k = k + increment

    return selected_features, data[selected_features]


  def integrating_data(self, index_name):
    data = self.data
    omics = []
    df_integrated_omics= pd.DataFrame()
    for df_data in data:
      for i in range(0,self.iterations):
        features, df_features = self.features_by_bagging(df_data)
        df_data = df_features
      df_features = resetIndex(df_features,index_name)
      omics.append(df_features)

    for index, single_omics in enumerate(omics):
      if index == 0:
        df_integrated_omics = single_omics
      else:
        df_integrated_omics = pd.merge(df_integrated_omics,single_omics, on=index_name)


    return omics, df_integrated_omics